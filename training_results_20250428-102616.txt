Training started at 20250428-102616
Model: GINNet
Batch size: 128
Learning rate: 0.0005
Weight decay: 1e-05
Device: cuda

Epoch,Train Loss,Val MAE
1,1.370100,0.881437
2,0.894890,0.849527
3,0.843947,0.823551
4,0.820086,0.741732
5,0.807611,0.734705
6,0.794966,0.742115
7,0.791164,0.702173
8,0.774401,0.703403
9,0.769477,0.695464
10,0.763208,0.721640
11,0.751700,0.708657
12,0.747868,0.677289
13,0.741568,0.700410
14,0.740122,0.668529
15,0.734310,0.675500
16,0.729414,0.665009
17,0.718426,0.662712
18,0.713251,0.656993
19,0.708800,0.666333
20,0.705998,0.695251
21,0.697490,0.640905
22,0.691304,0.643944
23,0.686661,0.634246
24,0.685004,0.627320
25,0.685238,0.635814
26,0.684203,0.639762
27,0.679193,0.639308
28,0.669801,0.636976
29,0.667873,0.621707
30,0.667082,0.643136
31,0.662503,0.613317
32,0.660207,0.607565
33,0.656910,0.619697
34,0.654026,0.605277
35,0.654345,0.601881
36,0.647315,0.609002
37,0.644793,0.606834
38,0.641246,0.605075
39,0.633740,0.599268
40,0.640300,0.587042
41,0.642153,0.612070
42,0.637636,0.633239
43,0.632949,0.608271
44,0.634863,0.586238
45,0.623236,0.578275
46,0.625023,0.590487
47,0.621701,0.595203
48,0.621268,0.582097
49,0.618922,0.635808
50,0.621482,0.585093
51,0.615263,0.592525
52,0.604107,0.587826
53,0.604238,0.572869
54,0.602743,0.605172
55,0.602523,0.565755
56,0.599793,0.571187
57,0.599342,0.576750
58,0.595506,0.569545
59,0.596056,0.577290
60,0.592314,0.571217
61,0.592029,0.579549
62,0.583850,0.558368
63,0.579473,0.570157
64,0.579396,0.555176
65,0.577997,0.556677
66,0.578274,0.556565
67,0.574207,0.558925
68,0.571450,0.560622
69,0.576446,0.545486
70,0.575270,0.571976
71,0.566983,0.548768
72,0.573395,0.551539
73,0.571007,0.550879
74,0.572218,0.553451
75,0.569863,0.546400
76,0.563627,0.554101
77,0.561501,0.549607
78,0.558197,0.548309
79,0.556859,0.560281
80,0.557559,0.549141
81,0.555766,0.551906
82,0.554149,0.547030
83,0.554660,0.547260
84,0.548703,0.546258
85,0.551743,0.546643
86,0.548845,0.538883
87,0.551073,0.551616
88,0.548877,0.547674
89,0.549551,0.545014
90,0.548634,0.540393
91,0.541781,0.542094
92,0.547008,0.540902
93,0.545790,0.540976
94,0.545098,0.538874
95,0.539766,0.537443
96,0.542580,0.538247
97,0.542356,0.540225
98,0.542990,0.547033
99,0.537969,0.539467
100,0.539977,0.539254
101,0.541751,0.535077
102,0.537540,0.539901
103,0.539803,0.539912
104,0.538055,0.533608
105,0.538629,0.542220
106,0.541585,0.539715
107,0.535604,0.535539
108,0.535143,0.536364
109,0.537038,0.537751
110,0.535608,0.531003
111,0.535499,0.535238
112,0.530603,0.530137
113,0.537105,0.539000
114,0.532211,0.535581
115,0.532818,0.537928
116,0.534784,0.538447
117,0.531387,0.535268
118,0.531466,0.536291
119,0.531109,0.531175
120,0.528044,0.536529
121,0.525984,0.538701
122,0.524819,0.539365
123,0.529497,0.538493
124,0.521366,0.537660
125,0.526772,0.535906
126,0.525824,0.534303
127,0.521674,0.533145
128,0.530326,0.533451
129,0.525258,0.532332
130,0.527086,0.533429
131,0.521933,0.529585
132,0.521921,0.534176
133,0.525064,0.534090
134,0.522133,0.538091
135,0.522248,0.529857
136,0.523678,0.535140
137,0.522114,0.530501
138,0.522126,0.530321
139,0.523949,0.530602
140,0.521269,0.530315
141,0.516096,0.533557
142,0.517507,0.529225
143,0.517403,0.534346
144,0.523584,0.532370
145,0.520461,0.535200
146,0.516408,0.529955
147,0.520133,0.532061
148,0.519834,0.534003
149,0.518619,0.531254
150,0.520359,0.533235
151,0.516639,0.532263
152,0.518687,0.531264
153,0.517120,0.528966
154,0.513424,0.530196
155,0.515801,0.530489
156,0.519999,0.532376
157,0.516801,0.530439
158,0.514795,0.530015
159,0.521113,0.532781
160,0.519334,0.531672
161,0.517244,0.532150
162,0.517826,0.532054
163,0.516493,0.533305
164,0.514923,0.534486
165,0.517104,0.534954
166,0.515097,0.533031
167,0.516643,0.529744
168,0.517022,0.538142
169,0.516665,0.530156
170,0.513982,0.529324
171,0.517044,0.533837
172,0.516727,0.535259
173,0.515745,0.532673
